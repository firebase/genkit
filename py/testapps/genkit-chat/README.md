# Genkit Chat

A full-stack multi-model AI chat application showcasing all Python Genkit features.

## Features

- ğŸ’¬ **Multi-Model Chat**: Switch between Google AI, Anthropic, OpenAI, Ollama, and more
- ğŸ”€ **Model Comparison**: Compare responses from multiple models side-by-side
- ğŸ™ï¸ **Voice Input/Output**: Speech-to-text and text-to-speech support
- ğŸ“ **File Upload**: Drag-and-drop images and documents
- ğŸ”§ **Genkit DevUI**: Test all flows and prompts at localhost:4000
- ğŸŒ™ **Dark/Light Theme**: Beautiful Material Design with theme toggle
- ğŸ“¦ **Containerized**: Podman/Docker compatible for Cloud Run deployment

## Genkit Features Demonstrated

This sample exercises all major Python Genkit capabilities:

| Feature | Implementation |
|---------|----------------|
| **Flows** | `chat_flow`, `compare_flow`, `describe_image_flow`, `rag_flow` |
| **Tools** | `web_search`, `get_weather`, `calculate`, `get_current_time` |
| **Prompts** | `.prompt` files in `backend/prompts/` |
| **Streaming** | `stream_chat_flow` with SSE |
| **RAG** | ChromaDB integration in `rag_flow` |
| **Multimodal** | Image description with vision models |

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              GENKIT CHAT                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    Angular Frontend (Port 4200/49230)               â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚   â”‚  â”‚ ChatService â”‚  â”‚ ModelService â”‚  â”‚       Components           â”‚  â”‚   â”‚
â”‚   â”‚  â”‚  - messages â”‚  â”‚  - providers â”‚  â”‚  â”œâ”€â”€ ChatComponent         â”‚  â”‚   â”‚
â”‚   â”‚  â”‚  - send()   â”‚  â”‚  - models    â”‚  â”‚  â”œâ”€â”€ CompareComponent      â”‚  â”‚   â”‚
â”‚   â”‚  â”‚  - compare()â”‚  â”‚  - fetch()   â”‚  â”‚  â””â”€â”€ SettingsComponent     â”‚  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚   â”‚         â”‚                â”‚                                           â”‚   â”‚
â”‚   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚   â”‚
â”‚   â”‚                  â”‚ HTTP/SSE                                          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                      â”‚                                                       â”‚
â”‚                      â”‚ Proxy (/api â†’ :8000)                                  â”‚
â”‚                      â–¼                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    Python Backend (Port 8000)                       â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚  â”‚                    HTTP Server (--framework robyn|fastapi)        â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  POST /api/chat       â†’  chat_flow                            â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  POST /api/compare    â†’  compare_flow                         â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  GET  /api/models     â†’  get_available_models()               â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  POST /api/stream     â†’  stream_chat_flow (SSE)               â”‚   â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚                              â”‚                                       â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚  â”‚                    Genkit Framework                           â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚ Flows                                                  â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚  â”œâ”€â”€ chat_flow         (single model chat)            â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚  â”œâ”€â”€ compare_flow      (multi-model comparison)       â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚  â”œâ”€â”€ stream_chat_flow  (streaming responses)          â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚  â”œâ”€â”€ describe_image    (vision/multimodal)            â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚  â””â”€â”€ rag_flow          (retrieval-augmented gen)      â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚   Tools     â”‚  â”‚  Prompts    â”‚  â”‚    Plugins          â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚ â”œâ”€ search   â”‚  â”‚ â”œâ”€ chat     â”‚  â”‚ â”œâ”€ google_genai     â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚ â”œâ”€ weather  â”‚  â”‚ â”œâ”€ rag      â”‚  â”‚ â”œâ”€ anthropic        â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚ â”œâ”€ calculateâ”‚  â”‚ â”œâ”€ compare  â”‚  â”‚ â”œâ”€ openai           â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â”‚ â””â”€ time     â”‚  â”‚ â””â”€ describe â”‚  â”‚ â”œâ”€ ollama           â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â””â”€ chroma (RAG)     â”‚   â”‚   â”‚   â”‚
â”‚   â”‚  â”‚                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                              â–¼                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                       Model Providers                               â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚   â”‚  â”‚ Google AIâ”‚ â”‚ Anthropicâ”‚ â”‚  OpenAI  â”‚ â”‚  Ollama  â”‚ â”‚ Vertex AIâ”‚  â”‚   â”‚
â”‚   â”‚  â”‚  Gemini  â”‚ â”‚  Claude  â”‚ â”‚   GPT    â”‚ â”‚  Llama   â”‚ â”‚  Gemini  â”‚  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

### Chat Message Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User     â”‚     â”‚   Frontend  â”‚     â”‚   Backend   â”‚     â”‚    Model    â”‚
â”‚             â”‚     â”‚  (Angular)  â”‚     â”‚  (Python)   â”‚     â”‚  Provider   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚  Type message     â”‚                   â”‚                   â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚  POST /api/chat   â”‚                   â”‚
       â”‚                   â”‚  {message, model, â”‚                   â”‚
       â”‚                   â”‚   history}        â”‚                   â”‚
       â”‚                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚                   â”‚  g.generate()     â”‚
       â”‚                   â”‚                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚                   â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚                   â”‚                   â”‚  LLM response     â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
       â”‚                   â”‚  {response, model,â”‚                   â”‚
       â”‚                   â”‚   latency_ms}     â”‚                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚                   â”‚
       â”‚  Display response â”‚                   â”‚                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
```

### Model Comparison Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User     â”‚     â”‚   Frontend  â”‚     â”‚   Backend   â”‚     â”‚   Models    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚  Enter prompt,    â”‚                   â”‚                   â”‚
       â”‚  select models    â”‚                   â”‚                   â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚ POST /api/compare â”‚                   â”‚
       â”‚                   â”‚ {prompt, models}  â”‚                   â”‚
       â”‚                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚                   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
       â”‚                   â”‚                   â”‚ â”‚   Parallel    â”‚ â”‚
       â”‚                   â”‚                   â”‚ â”‚  Generation   â”‚ â”‚
       â”‚                   â”‚                   â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
       â”‚                   â”‚                   â”‚ â”‚ Model A â”€â”€â”€â”€â”€â”€â”¼â”€â”¤
       â”‚                   â”‚                   â”‚ â”‚ Model B â”€â”€â”€â”€â”€â”€â”¼â”€â”¤
       â”‚                   â”‚                   â”‚ â”‚ Model C â”€â”€â”€â”€â”€â”€â”¼â”€â”¤
       â”‚                   â”‚                   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
       â”‚                   â”‚                   â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚                   â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
       â”‚                   â”‚  {responses: [...]}â”‚                  â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚                   â”‚
       â”‚  Side-by-side     â”‚                   â”‚                   â”‚
       â”‚  comparison       â”‚                   â”‚                   â”‚
```

### Streaming Response Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User     â”‚     â”‚   Frontend  â”‚     â”‚   Backend   â”‚     â”‚    Model    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚  Send message     â”‚                   â”‚                   â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚ POST /api/stream  â”‚                   â”‚
       â”‚                   â”‚ (SSE connection)  â”‚                   â”‚
       â”‚                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚                   â”‚  stream_generate()â”‚
       â”‚                   â”‚                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚  data: chunk1     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚  data: chunk2     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚                   â”‚  data: chunk3     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
       â”‚                   â”‚                   â”‚                   â”‚
       â”‚  Real-time typing â”‚  data: [DONE]     â”‚                   â”‚
       â”‚  effect!          â”‚                   â”‚                   â”‚
```



### Prerequisites

- Python 3.10+
- Node.js 24+ (use [fnm](https://github.com/Schniz/fnm) for version management)
- `GEMINI_API_KEY` (or other model provider API key)

### Run with DevUI (Recommended)

```bash
# Set your API key
export GEMINI_API_KEY="your-api-key"

# Run with Genkit DevUI
./run.sh dev
```

This starts:
- **DevUI**: http://localhost:4000 (test flows and prompts)
- **API**: http://localhost:8080

### Run Backend Only

```bash
# Default: Robyn (fast Rust-based server)
./run.sh backend

# Or use FastAPI
./run.sh backend --framework fastapi
```

### Run with Different Frameworks

The backend supports two web frameworks:

| Framework | Command | Description |
|-----------|---------|-------------|
| **Robyn** (default) | `./run.sh start` | Fast Rust-based async server |
| **FastAPI** | `./run.sh start --framework fastapi` | Industry-standard Python framework |

You can use `--framework` with any backend command:
- `./run.sh start --framework fastapi` - Full stack with FastAPI
- `./run.sh dev --framework fastapi` - DevUI mode with FastAPI
- `./run.sh backend --framework fastapi` - Backend only with FastAPI

### Run Frontend Only

```bash
./run.sh frontend
```

Frontend runs at http://localhost:4200

## Project Structure

```
genkit-chat/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ main.py          # Genkit flows, tools, Robyn/FastAPI server
â”‚   â”œâ”€â”€ prompts/             # Dotprompt files
â”‚   â”‚   â”œâ”€â”€ chat.prompt
â”‚   â”‚   â”œâ”€â”€ describe_image.prompt
â”‚   â”‚   â”œâ”€â”€ rag.prompt
â”‚   â”‚   â””â”€â”€ compare.prompt
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chat/    # Main chat component
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ compare/ # Model comparison
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ settings/
â”‚   â”‚   â”‚   â””â”€â”€ core/
â”‚   â”‚   â”‚       â””â”€â”€ services/
â”‚   â”‚   â”‚           â”œâ”€â”€ chat.service.ts
â”‚   â”‚   â”‚           â”œâ”€â”€ models.service.ts
â”‚   â”‚   â”‚           â”œâ”€â”€ speech.service.ts
â”‚   â”‚   â”‚           â””â”€â”€ theme.service.ts
â”‚   â”‚   â””â”€â”€ styles.scss
â”‚   â”œâ”€â”€ angular.json
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ run.sh                   # Cross-platform run script
â”œâ”€â”€ Containerfile            # Podman/Docker build
â”œâ”€â”€ docker-compose.yml       # Local development with Ollama
â””â”€â”€ README.md
```

## Testing in DevUI

All flows are registered with Genkit and testable in the DevUI:

### chat_flow
```json
{"message": "Hello!", "model": "googleai/gemini-3-flash-preview"}
```

### compare_flow
```json
{
  "prompt": "Explain quantum computing",
  "models": ["googleai/gemini-3-flash-preview", "ollama/llama3.2"]
}
```

### describe_image_flow
```json
{
  "image_url": "https://example.com/image.jpg",
  "question": "What's in this image?"
}
```

### rag_flow
```json
{"query": "What is Genkit?", "collection": "documents"}
```

## Container Build

### Build with Podman

```bash
./run.sh container
# or
podman build -t genkit-chat:latest -f Containerfile .
```

### Run Container

```bash
podman run -p 8080:8080 \
  -e GEMINI_API_KEY="your-key" \
  genkit-chat:latest
```

## Deploy to Cloud Run

```bash
./run.sh deploy
# or
gcloud run deploy genkit-chat \
  --source . \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --set-env-vars GEMINI_API_KEY="your-key"
```

## Local Development with Ollama

For fully offline development:

```bash
# Start Ollama
ollama serve

# Pull models
ollama pull llama3.2
ollama pull mistral

# Run with Docker Compose (includes Ollama)
docker-compose up
```

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `GEMINI_API_KEY` | Yes* | Gemini API key (preferred) |
| `GOOGLE_GENAI_API_KEY` | No | Legacy fallback for Gemini API key |
| `ANTHROPIC_API_KEY` | No | Anthropic API key |
| `OPENAI_API_KEY` | No | OpenAI API key |
| `OLLAMA_HOST` | No | Ollama server (default: localhost:11434) |
| `PORT` | No | Server port (default: 8080) |

*At least one model provider is required.

## API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/models` | List available models |
| POST | `/api/chat` | Send chat message |
| POST | `/api/compare` | Compare multiple models |
| POST | `/api/images/describe` | Describe an image |
| POST | `/api/rag` | RAG query with ChromaDB |

## Debugging

### Testing with curl

Test the backend API directly with curl to debug issues:

```bash
# List available models
curl -s http://localhost:8080/api/models | jq

# Check API configuration
curl -s http://localhost:8080/api/config | jq

# Test chat endpoint with Gemini
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hello! Tell me a joke.",
    "model": "googleai/gemini-3-flash-preview",
    "history": []
  }' | jq

# Test chat with Ollama (local)
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is 2+2?",
    "model": "ollama/llama3.2",
    "history": []
  }' | jq

# Test model comparison
curl -X POST http://localhost:8080/api/compare \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain quantum computing in one sentence",
    "models": ["googleai/gemini-3-flash-preview", "ollama/llama3.2"]
  }' | jq
```

### Common Issues

**429 RESOURCE_EXHAUSTED**: API quota exceeded - wait a moment or use a different model

**Model not found (404)**: Pull the Ollama model first:
```bash
ollama pull llama3.2
ollama pull gemma3
```

**Tool validation error**: Check that tools are defined with Pydantic models (not primitive types)

**Frontend not connecting**: Ensure proxy config in `angular.json` points to port 8080

### Viewing Backend Logs

Run the backend directly to see detailed error logs:

```bash
cd backend
uv run python src/main.py
```

## License

Apache 2.0 - See [LICENSE](../../../LICENSE) for details.
